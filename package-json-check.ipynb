{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "from urllib.error import HTTPError\n",
    "import pip._vendor.requests as requests\n",
    "import os\n",
    "import pprint as pp\n",
    "import time\n",
    "\n",
    "\n",
    "def get_data_request(url):\n",
    "    GITHUB_KEY = os.environ[\"GITHUB_KEY\"]\n",
    "    headers = {}\n",
    "\n",
    "    if GITHUB_KEY:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_KEY}\"\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    print(r.raise_for_status)\n",
    "    data = r.json()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_next_page_link_request(url):\n",
    "    GITHUB_KEY = os.environ[\"GITHUB_KEY\"]\n",
    "    headers = {}\n",
    "\n",
    "    if GITHUB_KEY:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_KEY}\"\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "    link = r.headers.get(\"link\")\n",
    "\n",
    "    return link\n",
    "\n",
    "def print_rate_limits(url):\n",
    "\n",
    "    GITHUB_KEY = os.environ[\"GITHUB_KEY\"]\n",
    "    headers = {}\n",
    "\n",
    "    if GITHUB_KEY:\n",
    "        headers[\"Authorization\"] = f\"token {GITHUB_KEY}\"\n",
    "\n",
    "    r = requests.get(url, headers=headers)\n",
    "\n",
    "    rate_limit = r.headers[\"X-RateLimit-Limit\"]\n",
    "    print(f'Requests allowed per minute: {rate_limit}')\n",
    "\n",
    "    rate_limit_reset = r.headers[\"X-RateLimit-Reset\"]\n",
    "    print(f'When rate limit will reset (unix): {rate_limit_reset}')\n",
    "\n",
    "    rate_limit_remaining = r.headers[\"X-RateLimit-Remaining\"]\n",
    "    print(f'Requests remaining for current minute: {rate_limit_remaining}')\n",
    "\n",
    "    rate_limit_used = r.headers[\"X-RateLimit-Used\"]\n",
    "    print(f'Request used in current minute: {rate_limit_used}')\n",
    "\n",
    "\n",
    "def write_dependency_file(url, absolute_path):\n",
    "    data = get_data_request(url)\n",
    "\n",
    "    file_content = data[\"content\"]\n",
    "    file_content_encoding = data.get(\"encoding\")\n",
    "    if file_content_encoding == \"base64\":\n",
    "        file_content = base64.b64decode(file_content).decode()\n",
    "    f = open(absolute_path, \"w\")\n",
    "    f.write(file_content)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def create_dependency_subdirectory(dependency_path, repository_name):\n",
    "    path_split = dependency_path.split(\"/\")  # split path string into list\n",
    "\n",
    "    remove_dependency_file = path_split[\n",
    "        :-1\n",
    "    ]  # remove dependency file from filepath in order to create subdir only filepath\n",
    "\n",
    "    relative_subdir_path = \"/\".join(\n",
    "        remove_dependency_file\n",
    "    )  # rejoin to list elements to create string with subdir path\n",
    "\n",
    "    absolute_subdir_path = os.path.join(\n",
    "        f\"repos/{repository_name}/{relative_subdir_path}\"\n",
    "    )  # create absolute path from root repos folder\n",
    "\n",
    "    # Check if folder has already been created, if not, create it\n",
    "    if not os.path.exists(absolute_subdir_path):\n",
    "        os.makedirs(absolute_subdir_path)\n",
    "\n",
    "    return absolute_subdir_path\n",
    "\n",
    "\n",
    "def search_dependency_files(username, repository_name, dependency_file):\n",
    "\n",
    "    print(f'Searching {repository_name} for {dependency_file}...')\n",
    "    # Retrieve all dependency file locations within root and all subdirectories of repo\n",
    "\n",
    "    url = f\"https://api.github.com/search/code?q=filename:{dependency_file}+org:{username}+repo:{username}/{repository_name}&page=1&per_page=100\"\n",
    "\n",
    "    print_rate_limits(url)\n",
    "\n",
    "    data = get_data_request(url)\n",
    "\n",
    "    # Loop through all dependency files locations\n",
    "\n",
    "    for item in data[\"items\"]:\n",
    "        dependency_path = item[\"path\"]\n",
    "        dependency_name = item[\"name\"]\n",
    "\n",
    "        if (\n",
    "            item[\"path\"] == dependency_file\n",
    "        ):  # If dependency file is inside project root directory (no need to generate subdirectories), write to file\n",
    "            url = f\"https://api.github.com/repos/{username}/{repository_name}/contents/{dependency_file}\"\n",
    "            absolute_path = f\"repos/{repository_name}/{dependency_path}\"\n",
    "            write_dependency_file(url, absolute_path)\n",
    "\n",
    "        else:  # if dependency files are nested in subdirectories\n",
    "            # Create nested subdirectories\n",
    "\n",
    "            absolute_subdir_path = create_dependency_subdirectory(\n",
    "                dependency_path, repository_name\n",
    "            )\n",
    "\n",
    "            # Write dependencies to file inside relevant nested subdirectory\n",
    "\n",
    "            url = item[\"url\"]\n",
    "            absolute_path = f\"{absolute_subdir_path}/{dependency_name}\"\n",
    "            write_dependency_file(url, absolute_path)\n",
    "\n",
    "\n",
    "def get_repo_names(username):\n",
    "    all_repo_names = []\n",
    "    page_number = 1\n",
    "    per_page = 100\n",
    "    pages_remaining = True\n",
    "    while_count = 0\n",
    "\n",
    "    # Make get request to fetch data of all repos and convert to JSON\n",
    "\n",
    "    while pages_remaining:\n",
    "        current_url = f\"https://api.github.com/users/{username}/repos?page={page_number}&per_page={per_page}\"\n",
    "        print(\"\")\n",
    "        print(f\"while_count - {while_count}\")\n",
    "\n",
    "        data = get_data_request(current_url)\n",
    "\n",
    "        # Loop through data JSON object to extract all repo names\n",
    "\n",
    "        for i in range(0, len(data)):\n",
    "            all_repo_names.append(data[i][\"name\"])\n",
    "        print(all_repo_names)\n",
    "\n",
    "        nextPattern = 'rel=\"next\"'\n",
    "        link = get_next_page_link_request(current_url)\n",
    "\n",
    "        if link and nextPattern in link:\n",
    "            print(\"Found next pattern in link, continue to next page\")\n",
    "            page_number += 1\n",
    "            while_count += 1\n",
    "            print(current_url)\n",
    "        else:\n",
    "            print(\"Can't find next pattern, must be last page\")\n",
    "            pages_remaining = False\n",
    "\n",
    "    pp.pprint(all_repo_names)\n",
    "\n",
    "    return all_repo_names\n",
    "\n",
    "def create_project_directories(all_repo_names):\n",
    "    \n",
    "    for i in range(0, len(all_repo_names)):\n",
    "\n",
    "        # Create make filepath to create new repo directory:\n",
    "\n",
    "        path = os.path.join(\"repos\", all_repo_names[i])\n",
    "\n",
    "        # Create repo subdirectories\n",
    "\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "\n",
    "def extract_repo_dependencies(all_repo_names, dependency_files, username, time_sleep):\n",
    "\n",
    "    # Search for dependency files within project subdirectories\n",
    "\n",
    "    for i in range(0, len(all_repo_names)):\n",
    "        time.sleep(time_sleep)\n",
    "        print(\"\")\n",
    "        for j in range(0, len(dependency_files)):\n",
    "            search_dependency_files(username, all_repo_names[i], dependency_files[j])\n",
    "\n",
    "\n",
    "def main():\n",
    "    USERNAME = \"Harrisman05\"\n",
    "    DEPENDENCY_FILES = [\"package.json\", \"requirements.txt\"]\n",
    "    TIME_SLEEP = 15\n",
    "\n",
    "    ALL_REPO_NAMES = get_repo_names(USERNAME)\n",
    "    create_project_directories(ALL_REPO_NAMES)\n",
    "    extract_repo_dependencies(ALL_REPO_NAMES, DEPENDENCY_FILES, USERNAME, TIME_SLEEP)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Github Repo Dependency Checker\n",
    "\n",
    "    # Inputs needed from user are:\n",
    "\n",
    "    # USERNAME: A string containing a github username\n",
    "\n",
    "    # DEPENDENCY_FILES: A list of strings that are names of the dependency files that need to be scanned within each repository. 'package.json' has been tested (and will also pull relevant package-lock.json) as well as requirements.txt\n",
    "\n",
    "    # TIME_SLEEP: An integer that determines the speed of the loops executed to recursively pull data from the repositories. The higher the number of DEPENDENCY_FILES and the more repos, the higher the time sleep value needs to be to avoid rate limiting. Start with 5, and if a HTTP Forbidden Link error is thrown, increase TIME_SLEEP. 30 requests per minute are allowed\n",
    "\n",
    "    # ALL_REPO_NAMES: A list of string containing all the visible repos in a user's github\n",
    "\n",
    "        # How the code works\n",
    "\n",
    "            # 1) Obtain all repo names from a user. This code is wrapped in a while loop as the Github API can only load 100 repos per get request at a maximum, so the loop allows for automatic navigation to the next page until no pages remain. For example, a user with 720 repos will have 8 pages worth of repos\n",
    "\n",
    "            # 2) Create repo directories within 'repos' for each repo name. For example, a user with 82 repos will have 82 subdirectories in repos\n",
    "\n",
    "            # 3) Retrieve all dependency files locations (in root and in nested subdirectories) of a repo with a single request\n",
    "\n",
    "            # 4) Loop through all the dependency file locations:\n",
    "\n",
    "                # If the dependency files are in the project root, then make a get request to extract the contents of the dependency file and then write to file at repos/{repo_name}\n",
    "\n",
    "                # If the dependency files are nested in subdirectories:\n",
    "\n",
    "                    # Create the dependency path and then the subdirectories. This will maintain the structure of nested dependency files\n",
    "\n",
    "                    # Then make the request to extract contents and then write the dependency to file inside the relevant subdirectory. Again, this will maintain project structure\n",
    "\n",
    "# Note, if a project doesn't have any dependencies, an empty subdirectory with the project name will just be created."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dependency_checker",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6bb2c615e6571d5756fb47ecc774e74876f593876d64f7be0c48842ff5d06f09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
